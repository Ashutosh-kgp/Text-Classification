{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import string\n",
    "import nltk\n",
    "# import wordlist\n",
    "import warnings \n",
    "from nltk.stem.porter import *\n",
    "from sklearn.svm import SVC\n",
    "import pickle \n",
    "from word_list import unique_word_list\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    " \n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopword_list + unique_word_list\n",
    "\n",
    "def pre_processing(text):\n",
    " \n",
    "    expr = re.compile('\\d{2}/\\d{2}/\\d{4}')\n",
    "    line = re.sub(expr,'',text )\n",
    "    expr = re.compile('\\d{2}:\\d{2}:\\d{2}')\n",
    "    text = re.sub(expr, '', line)\n",
    "    text = re.sub(\"\\d\", \"\", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pattern = re.compile('[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\.:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~]')\n",
    "    tokens = [w for w in tokens if w not in stopword_list]\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    " \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_convert(predictions):\n",
    "    \n",
    "    int_predict = []\n",
    "    \n",
    "    for i in range(0,len(predictions)):\n",
    "        \n",
    "        result = np.where(predictions[i] == np.amax(predictions[i]))\n",
    "\n",
    "        int_predict.append(result[0][0])\n",
    "        \n",
    "    return int_predict\n",
    "\n",
    "def inference(input_query):\n",
    "     #Load file weight\n",
    "     \n",
    "        \n",
    "    filename = 'weights/lbl_enc.sav'\n",
    "    lbl_enc= pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    filename = 'weights/tfv_weights.sav'\n",
    "    tfv = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    filename = 'weights/svd_weights.sav'\n",
    "    svd = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    filename = 'weights/scl_weights.sav'\n",
    "    scl = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    filename = 'weights/clf_weights.sav'\n",
    "    clf = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    input_query = tfv.transform(input_query)\n",
    "    input_query = svd.transform(input_query)\n",
    "    input_query = scl.transform(input_query)\n",
    "    \n",
    "   \n",
    "     \n",
    "    predictions = clf.predict_proba(input_query)\n",
    "     \n",
    "    \n",
    "    int_predict = integer_convert(predictions)\n",
    "     \n",
    "    labels =  lbl_enc.inverse_transform(int_predict)\n",
    "    \n",
    "    print('Label:{}'.format(labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:['EMSE SMARTS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashutosh/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.21.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ashutosh/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.21.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ashutosh/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.21.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ashutosh/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TruncatedSVD from version 0.21.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ashutosh/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator StandardScaler from version 0.21.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ashutosh/anaconda3/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator SVC from version 0.21.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "input_query = '''Test_here'''\n",
    "\n",
    "input_query = pre_processing(input_query)\n",
    "inference([input_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
